{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Introduction \n",
    "GANs are comprised of both generator and discriminator models. The generator is responsible for generating new samples from the domain, and the discriminator is responsible for classifying whether samples are real or fake (generated). Importantly, the performance of the discriminator model is used to update both the model weights of the discriminator itself and the generator model. This means that the generator never actually sees examples from the domain and is adapted based on how well the discriminator performs.\n",
    "\n",
    "This is a complex type of model both to understand and to train.\n",
    "\n",
    "One approach to better understand the nature of GAN models and how they can be trained is to develop a model from scratch for a very simple task.\n",
    "\n",
    "A simple task that provides a good context for developing a simple GAN from scratch is a one-dimensional function. This is because both real and generated samples can be plotted and visually inspected to get an idea of what has been learned. A simple function also does not require sophisticated neural network models, meaning the specific generator and discriminator models used on the architecture can be easily understood.\n",
    "\n",
    "\n",
    "## This model can be divided into Six parts. \n",
    "\n",
    "Select a One-Dimensional Function\n",
    "Define a Discriminator Model\n",
    "Define a Generator Model\n",
    "Training the Generator Model\n",
    "Evaluating the Performance of the GAN\n",
    "Complete Example of Training the GAN\n",
    "\n",
    "\n",
    "*We are taking the function of y=x^2 for the sake of simplicity* \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrimanator Model \n",
    "\n",
    "The model must take a sample from our problem, such as a vector with two elements, and output a classification prediction as to whether the sample is real or fake.\n",
    "\n",
    "This is a binary classification problem.\n",
    "\n",
    "Inputs: Sample with two real values.\n",
    "Outputs: Binary classification, likelihood the sample is real (or fake).\n",
    "\n",
    "\n",
    "The problem is very simple, meaning that we don’t need a complex neural network to model it.\n",
    "\n",
    "The discriminator model will have one hidden layer with 25 nodes and we will use the ReLU activation function and an appropriate weight initialization method called He weight initialization.\n",
    "\n",
    "The output layer will have one node for the binary classification using the sigmoid activation function.\n",
    "\n",
    "The model will minimize the binary cross entropy loss function, and the Adam version of stochastic gradient descent will be used because it is very effective.\n",
    "\n",
    "The define_discriminator() function below defines and returns the discriminator model. The function parameterizes the number of inputs to expect, which defaults to two."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could start training this model now with real examples with a class label of one and randomly generated samples with a class label of zero.\n",
    "\n",
    "There is no need to do this, but the elements we will develop will be useful later, and it helps to see that the discriminator is just a normal neural network model.\n",
    "\n",
    "First, we can update our generate_samples() function from the prediction section and call it generate_real_samples() and have it also return the output class labels for the real samples, specifically, an array of 1 values, where class=1 means real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n real samples with class labels\n",
    "def generate_real_samples(n):\n",
    "\t# generate inputs in [-0.5, 0.5]\n",
    "\tX1 = rand(n) - 0.5\n",
    "\t# generate outputs X^2\n",
    "\tX2 = X1 * X1\n",
    "\t# stack arrays\n",
    "\tX1 = X1.reshape(n, 1)\n",
    "\tX2 = X2.reshape(n, 1)\n",
    "\tX = hstack((X1, X2))\n",
    "\t# generate class labels\n",
    "\ty = ones((n, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create fake samples. The above code can recycled. \n",
    "Fake samples must not give correct result. Hence  we will generate random values in the range -1 and 1 for both elements of a sample. The output class label for all of these examples is 0.\n",
    "\n",
    "This function will act as our fake generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n fake samples with class labels\n",
    "def generate_fake_samples(n):\n",
    "\t# generate inputs in [-1, 1]\n",
    "\tX1 = -1 + rand(n) * 2\n",
    "\t# generate outputs in [-1, 1]\n",
    "\tX2 = -1 + rand(n) * 2\n",
    "\t# stack arrays\n",
    "\tX1 = X1.reshape(n, 1)\n",
    "\tX2 = X2.reshape(n, 1)\n",
    "\tX = hstack((X1, X2))\n",
    "\t# generate class labels\n",
    "\ty = zeros((n, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to train and evaluate the discriminator model.\n",
    "\n",
    "This can be achieved by manually enumerating the training epochs and for each epoch generating a half batch of real examples and a half batch of fake examples, and updating the model on each, e.g. one whole batch of examples. The train() function could be used, but in this case, we will use the train_on_batch() function directly.\n",
    "\n",
    "The model can then be evaluated on the generated examples and we can report the classification accuracy on the real and fake samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_discriminator() function below implements this, training the model for 1,000 batches and using 128 samples per batch (64 fake and 64 real).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the discriminator model\n",
    "def train_discriminator(model, n_epochs=1000, n_batch=128):\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# run epochs manually\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# generate real examples\n",
    "\t\tX_real, y_real = generate_real_samples(half_batch)\n",
    "\t\t# update model\n",
    "\t\tmodel.train_on_batch(X_real, y_real)\n",
    "\t\t# generate fake examples\n",
    "\t\tX_fake, y_fake = generate_fake_samples(half_batch)\n",
    "\t\t# update model\n",
    "\t\tmodel.train_on_batch(X_fake, y_fake)\n",
    "\t\t# evaluate the model\n",
    "\t\t_, acc_real = model.evaluate(X_real, y_real, verbose=0)\n",
    "\t\t_, acc_fake = model.evaluate(X_fake, y_fake, verbose=0)\n",
    "\t\tprint(i, acc_real, acc_fake)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary uptil now\n",
    "Running the example generates real and fake examples and updates the model, then evaluates the model on the same examples and prints the classification accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Generator Model\n",
    "\n",
    "The generator model takes as input a point from the latent space and generates a new sample, e.g. a vector with both the input and output elements of our function, e.g. x and x^2.\n",
    "\n",
    "A latent variable is a hidden or unobserved variable, and a latent space is a multi-dimensional vector space of these variables. We can define the size of the latent space for our problem and the shape or distribution of variables in the latent space.\n",
    "\n",
    "This is because the latent space has no meaning until the generator model starts assigning meaning to points in the space as it learns. After training, points in the latent space will correspond to points in the output space, e.g. in the space of generated samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a small latent space of five dimensions and use the standard approach in the GAN literature of using a Gaussian distribution for each variable in the latent space. We will generate new inputs by drawing random numbers from a standard Gaussian distribution, i.e. mean of zero and a standard deviation of one.\n",
    "\n",
    "Inputs: Point in latent space, e.g. a five-element vector of Gaussian random numbers.\n",
    "Outputs: Two-element vector representing a generated sample for our function (x and x^2).\n",
    "The generator model will be small like the discriminator model.\n",
    "\n",
    "It will have a single hidden layer with five nodes and will use the ReLU activation function and the He weight initialization. The output layer will have two nodes for the two elements in a generated vector and will use a linear activation function.\n",
    "\n",
    "\n",
    "A linear activation function is used because we know we want the generator to output a vector of real values and the scale will be [-0.5, 0.5] for the first element and about [0.0, 0.25] for the second element.\n",
    "\n",
    "\n",
    "The model is not compiled. The reason for this is that the generator model is not fit directly.\n",
    "\n",
    "The define_generator() function below defines and returns the generator model.\n",
    "\n",
    "The size of the latent dimension is parameterized in case we want to play with it later, and the output shape of the model is also parameterized, matching the function for defining the discriminator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "\tmodel.add(Dense(n_outputs, activation='linear'))\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# define the discriminator model\n",
    "model = define_generator(5)\n",
    "# summarize the model\n",
    "model.summary()\n",
    "# plot the model\n",
    "plot_model(model, to_file='generator_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the model is also created and we can see that the model expects a five-element point from the latent space as input and will predict a two-element vector as output.\n",
    "We can see that the model takes as input a random five-element vector from the latent space and outputs a two-element vector for our one-dimensional function.\n",
    "\n",
    "This model cannot do much at the moment. Nevertheless, we can demonstrate how to use it to generate samples. This is not needed, but again, some of these elements may be useful later.\n",
    "\n",
    "\n",
    "The first step is to generate new points in the latent space. We can achieve this by calling the randn() NumPy function for generating arrays of random numbers drawn from a standard Gaussian"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array of random numbers can then be reshaped into samples: that is n rows with five elements per row. The generate_latent_points() function below implements this and generates the desired number of points in the latent space that can be used as input to the generator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = randn(latent_dim * n)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 5\n",
    "# define the discriminator model\n",
    "model = define_generator(latent_dim)\n",
    "# generate and plot generated samples\n",
    "generate_fake_samples(model, latent_dim, 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example generates 100 random points from the latent space, uses this as input to the generator and generates 100 fake samples from our one-dimensional function domain.\n",
    "As the generator has not been trained, the generated points are complete rubbish, as we expect, but we can imagine that as the model is trained, these points will slowly begin to resemble the target function and its u-shape."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model \n",
    "\n",
    "The weights in the generator model are updated based on the performance of the discriminator model.\n",
    "\n",
    "When the discriminator is good at detecting fake samples, the generator is updated more, and when the discriminator model is relatively poor or confused when detecting fake samples, the generator model is updated less.\n",
    "\n",
    "\n",
    "The weights in the generator model are updated based on the performance of the discriminator model.\n",
    "\n",
    "When the discriminator is good at detecting fake samples, the generator is updated more, and when the discriminator model is relatively poor or confused when detecting fake samples, the generator model is updated less."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make this concrete.\n",
    "\n",
    "Inputs: Point in latent space, e.g. a five-element vector of Gaussian random numbers.\n",
    "Outputs: Binary classification, likelihood the sample is real (or fake).\n",
    "The define_gan() function below takes as arguments the already-defined generator and discriminator models and creates the new logical third model subsuming these two models. The weights in the discriminator are marked as not trainable, which only affects the weights as seen by the GAN model and not the standalone discriminator model.\n",
    "The GAN model then uses the same binary cross entropy loss function as the discriminator and the efficient Adam version of stochastic gradient descent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tdiscriminator.trainable = False\n",
    "\t# connect them\n",
    "\tmodel = Sequential()\n",
    "\t# add generator\n",
    "\tmodel.add(generator)\n",
    "\t# add the discriminator\n",
    "\tmodel.add(discriminator)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the discriminator not trainable is a clever trick in the Keras API.\n",
    "\n",
    "The trainable property impacts the model when it is compiled. The discriminator model was compiled with trainable layers, therefore the model weights in those layers will be updated when the standalone model is updated via calls to train_on_batch().\n",
    "The discriminator model was marked as not trainable, added to the GAN model, and compiled. In this model, the model weights of the discriminator model are not trainable and cannot be changed when the GAN model is updated via calls to train_on_batch()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wokring code \n",
    "# demonstrate creating the three models in the gan\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "\tmodel.add(Dense(n_outputs, activation='linear'))\n",
    "\treturn model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tdiscriminator.trainable = False\n",
    "\t# connect them\n",
    "\tmodel = Sequential()\n",
    "\t# add generator\n",
    "\tmodel.add(generator)\n",
    "\t# add the discriminator\n",
    "\tmodel.add(discriminator)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\treturn model\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 5\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# summarize gan model\n",
    "gan_model.summary()\n",
    "# plot gan model\n",
    "plot_model(gan_model, to_file='gan_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the model is also created and we can see that the model expects a five-element point in latent space as input and will predict a single output classification label.\n",
    "Training the composite model involves generating a batch-worth of points in the latent space via the generate_latent_points() function in the previous section, and class=1 labels and calling the train_on_batch() function.\n",
    "The train_gan() function below demonstrates this, although it is pretty uninteresting as only the generator will be updated each epoch, leaving the discriminator with default model weights."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
